{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce code effectue plusieurs étapes de prétraitement des données, y compris l'imputation des valeurs manquantes, l'encodage ordinal des variables catégorielles, l'encodage one-hot des variables catégorielles avec plus d'une catégorie, et l'application d'une transformation Smooth Ridit aux variables numériques. Ensuite, il entraîne deux modèles, une régression ElasticNet et un modèle LightGBM avec une perte de Poisson, sur les données prétraitées. Enfin, il sauvegarde le modèle et le mapping des catégories d'attaques pour une utilisation ultérieure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\CYBERSENTINEL\\ML\\Modèle2.ipynb Cellule 2\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CYBERSENTINEL/ML/Mod%C3%A8le2.ipynb#W1sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     X_val_imputed \u001b[39m=\u001b[39m imputer\u001b[39m.\u001b[39mtransform(X_val_scaled)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CYBERSENTINEL/ML/Mod%C3%A8le2.ipynb#W1sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     enet \u001b[39m=\u001b[39m ElasticNet(alpha\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, l1_ratio\u001b[39m=\u001b[39m\u001b[39m0.7\u001b[39m, max_iter\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/CYBERSENTINEL/ML/Mod%C3%A8le2.ipynb#W1sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m enet\u001b[39m.\u001b[39mfit(X_train_imputed, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CYBERSENTINEL/ML/Mod%C3%A8le2.ipynb#W1sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m enet_pred_train \u001b[39m=\u001b[39m enet\u001b[39m.\u001b[39mpredict(X_train_imputed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/CYBERSENTINEL/ML/Mod%C3%A8le2.ipynb#W1sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m enet_pred_val \u001b[39m=\u001b[39m enet\u001b[39m.\u001b[39mpredict(X_val_imputed)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'enet' is not defined"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_poisson_deviance\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Configuration de la journalisation\n",
    "logging.basicConfig(filename='model.log', level=logging.INFO)\n",
    "\n",
    "def load_data(filepath):\n",
    "    logging.info(\"Chargement des données...\")\n",
    "    data = pd.read_csv(filepath, encoding='ISO-8859-1')\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    logging.info(\"Prétraitement des données...\")\n",
    "    # Imputer les valeurs manquantes pour les variables numériques\n",
    "    numeric_features = data.select_dtypes(include=['float64']).columns\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    data[numeric_features] = imputer.fit_transform(data[numeric_features])\n",
    "\n",
    "    # Appliquer la transformation Smooth Ridit aux caractéristiques numériques dans le DataFrame d'entraînement\n",
    "    numeric_features = data.select_dtypes(include=[np.number]).columns\n",
    "    data[numeric_features] = smooth_ridit_transform(data[numeric_features])\n",
    "\n",
    "    # Ordinal encoding pour les variables catégorielles\n",
    "    categorical_features = data.select_dtypes(include=['object']).columns\n",
    "    ordinal_encoder = OrdinalEncoder()\n",
    "    data[categorical_features] = ordinal_encoder.fit_transform(data[categorical_features])\n",
    "\n",
    "    # Vérifier le nombre de catégories uniques dans chaque colonne catégorielle\n",
    "    unique_counts = data[categorical_features].nunique()\n",
    "\n",
    "    # Filtrer les colonnes qui ont plus d'une catégorie\n",
    "    multi_cat_features = unique_counts[unique_counts > 1].index\n",
    "\n",
    "    # Appliquer l'encodage One-Hot uniquement sur les colonnes avec plus d'une catégorie\n",
    "    encoder = OneHotEncoder(sparse_output=False, drop='first', handle_unknown='ignore')\n",
    "    encoded_cat = encoder.fit_transform(data[multi_cat_features])\n",
    "    encoded_df = pd.DataFrame(encoded_cat, columns=encoder.get_feature_names_out(multi_cat_features))\n",
    "\n",
    "    # Supprimer les colonnes catégorielles originales\n",
    "    data.drop(multi_cat_features, axis=1, inplace=True)\n",
    "\n",
    "    # Concaténer les données encodées avec les données originales\n",
    "    df_encoded = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "    return df_encoded\n",
    "\n",
    "def train_model(df_encoded, target_column):\n",
    "    logging.info(\"Entraînement du modèle...\")\n",
    "    # Régression ElasticNet pour les variables numériques et catégorielles combinées\n",
    "    X = df_encoded.drop(target_column, axis=1)\n",
    "    y = df_encoded[target_column]\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X_train_imputed = imputer.fit_transform(X_train_scaled)\n",
    "    X_val_imputed = imputer.transform(X_val_scaled)\n",
    "    enet = ElasticNet(alpha=0.5, l1_ratio=0.7, max_iter=10000)\n",
    "enet.fit(X_train_imputed, y_train)\n",
    "enet_pred_train = enet.predict(X_train_imputed)\n",
    "enet_pred_val = enet.predict(X_val_imputed)\n",
    "mse = mean_squared_error(y_val, enet_pred_val)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# LightGBM avec perte de Poisson\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_val, label=y_val)\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'poisson',\n",
    "    'metric': 'poisson',\n",
    "    'learning_rate': 0.11,\n",
    "}\n",
    "\n",
    "# Définir le callback early_stopping\n",
    "early_stopping = lgb.callback.early_stopping(stopping_rounds=50, verbose=True)\n",
    "model = lgb.train(params, dtrain, num_boost_round=1000, valid_sets=[dval], callbacks=[early_stopping])\n",
    "\n",
    "y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "poisson_deviance = mean_poisson_deviance(y_val, y_pred)\n",
    "print(f\"Poisson Deviance: {poisson_deviance}\")\n",
    "\n",
    "# Sauvegarder le modèle et le mapping\n",
    "joblib.dump(model, 'model.pkl')\n",
    "joblib.dump(mapping, 'mapping.pkl')\n",
    "\n",
    "# Charger le modèle et le mapping\n",
    "model = joblib.load('model.pkl')\n",
    "mapping = joblib.load('mapping.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le DataFrame a 44.\n"
     ]
    }
   ],
   "source": [
    "nombre_de_colonnes = len(train.columns)\n",
    "\n",
    "# Afficher le nombre de colonnes\n",
    "print(f\"Le DataFrame a {nombre_de_colonnes} colonnes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes avant la transformation : Index(['sport', 'dsport', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss',\n",
      "       'dloss', 'sload', 'dload', 'spkts', 'dpkts', 'swin', 'dwin', 'stcpb',\n",
      "       'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'sjit',\n",
      "       'djit', 'stime', 'ltime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack',\n",
      "       'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
      "       'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'attack_cat', 'label'],\n",
      "      dtype='object')\n",
      "Colonnes après la transformation : Index(['sport', 'dsport', 'dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss',\n",
      "       'dloss', 'sload', 'dload', 'spkts', 'dpkts', 'swin', 'dwin', 'stcpb',\n",
      "       'dtcpb', 'smeansz', 'dmeansz', 'trans_depth', 'res_bdy_len', 'sjit',\n",
      "       'djit', 'stime', 'ltime', 'sintpkt', 'dintpkt', 'tcprtt', 'synack',\n",
      "       'ackdat', 'is_sm_ips_ports', 'ct_state_ttl', 'ct_flw_http_mthd',\n",
      "       'is_ftp_login', 'ct_ftp_cmd', 'ct_srv_src', 'ct_srv_dst', 'ct_dst_ltm',\n",
      "       'ct_src_ltm', 'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
      "       'attack_cat', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Appliquez votre transformation ici\n",
    "print(f\"Colonnes après la transformation : {train.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colonnes manquantes : set()\n"
     ]
    }
   ],
   "source": [
    "# Colonnes originales\n",
    "colonnes_originales = train.columns\n",
    "\n",
    "# Colonnes après la transformation\n",
    "colonnes_transformees = df_encoded.columns\n",
    "\n",
    "# Colonnes manquantes\n",
    "colonnes_manquantes = set(colonnes_originales) - set(colonnes_transformees)\n",
    "\n",
    "# Afficher les colonnes manquantes\n",
    "print(f\"Colonnes manquantes : {colonnes_manquantes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_columns = X_train.columns\n",
    "new_data_columns = new_data.columns\n",
    "missing_columns = set(original_columns) - set(new_data_columns)\n",
    "extra_columns = set(new_data_columns) - set(original_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\preprocessing\\_encoders.py:228: UserWarning: Found unknown categories in columns [0, 1, 2, 3, 4] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "C:\\Users\\steph\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:458: UserWarning: X has feature names, but SimpleImputer was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 155 features, but SimpleImputer is expecting 187 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\M2i\\RNCP\\RNCP_CDC\\Modèle2.ipynb Cellule 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m new_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([new_data, encoded_df_new_data], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Prétraiter new_data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m new_data_preprocessed \u001b[39m=\u001b[39m preprocess_data(new_data, imputer, ordinal_encoder, scaler, original_columns)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Faire des prédictions sur new_data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m new_data_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(new_data_preprocessed)\n",
      "\u001b[1;32me:\\M2i\\RNCP\\RNCP_CDC\\Modèle2.ipynb Cellule 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# Imputer les valeurs manquantes\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m numeric_features \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mselect_dtypes(include\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mfloat64\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m data[numeric_features] \u001b[39m=\u001b[39m imputer\u001b[39m.\u001b[39;49mtransform(data[numeric_features])\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Appliquer la transformation Smooth Ridit\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m numeric_features \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mselect_dtypes(include\u001b[39m=\u001b[39m[np\u001b[39m.\u001b[39mnumber])\u001b[39m.\u001b[39mcolumns\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:157\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    156\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 157\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    158\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    160\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    161\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    162\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    163\u001b[0m         )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\impute\\_base.py:530\u001b[0m, in \u001b[0;36mSimpleImputer.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Impute all missing values in `X`.\u001b[39;00m\n\u001b[0;32m    516\u001b[0m \n\u001b[0;32m    517\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    526\u001b[0m \u001b[39m    `X` with imputed values.\u001b[39;00m\n\u001b[0;32m    527\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    528\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 530\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_input(X, in_fit\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    531\u001b[0m statistics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatistics_\n\u001b[0;32m    533\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m statistics\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\impute\\_base.py:332\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[39mraise\u001b[39;00m new_ve \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    331\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 332\u001b[0m         \u001b[39mraise\u001b[39;00m ve\n\u001b[0;32m    334\u001b[0m \u001b[39mif\u001b[39;00m in_fit:\n\u001b[0;32m    335\u001b[0m     \u001b[39m# Use the dtype seen in `fit` for non-`fit` conversion\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_dtype \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\impute\\_base.py:315\u001b[0m, in \u001b[0;36mSimpleImputer._validate_input\u001b[1;34m(self, X, in_fit)\u001b[0m\n\u001b[0;32m    312\u001b[0m     force_all_finite \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 315\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    316\u001b[0m         X,\n\u001b[0;32m    317\u001b[0m         reset\u001b[39m=\u001b[39;49min_fit,\n\u001b[0;32m    318\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    319\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    320\u001b[0m         force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[0;32m    321\u001b[0m         copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy,\n\u001b[0;32m    322\u001b[0m     )\n\u001b[0;32m    323\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m ve:\n\u001b[0;32m    324\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcould not convert\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(ve):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:626\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 626\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    628\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:415\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 415\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    416\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    417\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 155 features, but SimpleImputer is expecting 187 features as input."
     ]
    }
   ],
   "source": [
    "# Prédire sur de nouvelles données\n",
    "new_data = pd.read_csv('E:\\M2i\\RNCP\\RNCP_CDC\\DataBase\\sample_train5.csv')\n",
    "\n",
    "def preprocess_data(data, imputer, encoder, scaler, train_columns):\n",
    "    # Ajouter des fonctionnalités manquantes\n",
    "    missing_cols = set(train_columns) - set(data.columns)\n",
    "    for c in missing_cols:\n",
    "        data[c] = np.nan\n",
    "\n",
    "    # Assurer que l'ordre des colonnes est le même que dans les données d'entraînement\n",
    "    data = data[train_columns]\n",
    "\n",
    "    # Imputer les valeurs manquantes\n",
    "    numeric_features = data.select_dtypes(include=['float64']).columns\n",
    "    data[numeric_features] = imputer.transform(data[numeric_features])\n",
    "\n",
    "    # Appliquer la transformation Smooth Ridit\n",
    "    numeric_features = data.select_dtypes(include=[np.number]).columns\n",
    "    data[numeric_features] = smooth_ridit_transform(data[numeric_features])\n",
    "\n",
    "    # Encoder en One-Hot\n",
    "    categorical_features = data\n",
    "    \n",
    "# Encoder en One-Hot\n",
    "encoded_cat_new_data = encoder.transform(new_data[categorical_features])\n",
    "encoded_df_new_data = pd.DataFrame(encoded_cat_new_data, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Supprimer les colonnes catégorielles originales dans les nouvelles données\n",
    "new_data.drop(categorical_features, axis=1, inplace=True)\n",
    "\n",
    "# Concaténer les nouvelles colonnes One-Hot avec les données originales\n",
    "# Continuer le prétraitement des nouvelles données\n",
    "new_data = pd.concat([new_data, encoded_df_new_data], axis=1)\n",
    "\n",
    "# Prétraiter new_data\n",
    "new_data_preprocessed = preprocess_data(new_data, imputer, ordinal_encoder, scaler, original_columns)\n",
    "\n",
    "# Faire des prédictions sur new_data\n",
    "new_data_predictions = model.predict(new_data_preprocessed)\n",
    "\n",
    "# Convertir les prédictions en catégories d'attaques\n",
    "new_data_attack_cats = pd.Series(new_data_predictions).map(mapping)\n",
    "\n",
    "# Afficher les catégories d'attaques prédites\n",
    "print(new_data_attack_cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_data_preprocessed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\M2i\\RNCP\\RNCP_CDC\\Modèle2.ipynb Cellule 6\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Faire des prédictions sur new_data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m new_data_predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(new_data_preprocessed)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Convertir les prédictions en catégories d'attaques\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/M2i/RNCP/RNCP_CDC/Mod%C3%A8le2.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m new_data_attack_cats \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(new_data_predictions)\u001b[39m.\u001b[39mmap(mapping)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_data_preprocessed' is not defined"
     ]
    }
   ],
   "source": [
    "# Faire des prédictions sur new_data\n",
    "new_data_predictions = model.predict(new_data_preprocessed)\n",
    "\n",
    "# Convertir les prédictions en catégories d'attaques\n",
    "new_data_attack_cats = pd.Series(new_data_predictions).map(mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
